# NBSVM
Since I still receive a good number of emails about this project 4 years later,
I decided to put this code on github and write the
instructions better. The code itself is unchanged (still just as bad).

For technical details see [our paper](wang12simple.pdf) and
[our talk](wang12simple_slides.pdf).

```
@inproceedings{wang12simple, 
author = {Wang, Sida I. and Manning, Christopher D.}, 
booktitle = {Proceedings of the ACL}, 
title = {Baselines and Bigrams: Simple, Good Sentiment and Topic Classification}, 
year = {2012}, 
booktitle = {ACL (2)}, 
pages = {90-94} 
} 
```

## Running NBSVM
- Download the data and override the empty data directory in root: "data/rt10662/unigram_rts.mat"
- Go to src and run the script master.m to produce the results from the paper
- Results and details are logged in resultslog.txt and details.txt, respectively
- A table with all the results is printed, like:

```
AthR	XGraph	BbCrypt	CR	IMDB	MPQA	RT-2k	RTs	subj	
85.13	91.19	99.40	79.97	86.59	86.27	85.85	79.03	93.56	
84.99	89.96	99.29	79.76	83.55	85.29	83.45	77.94	92.58	
83.73	86.17	97.68	80.85	89.16	86.72	87.40	77.72	91.74	
82.61	85.14	98.29	79.02	86.95	86.15	86.25	76.23	90.84	
87.66	90.68	99.50	81.75	91.22	86.32	89.45	79.38	93.18	
87.94	91.19	99.70	80.45	88.29	85.25	87.80	78.05	92.40	
```

## The data
- [data](http://www.stanford.edu/~sidaw/projects/data_NB_ACL12.zip) - 404.4MB includes all the data
- [data_small](http://www.stanford.edu/~sidaw/projects/datasmall_NB_ACL12.zip) - 108.5MB
  data_small = data_all - large_IMDB
- For each data set, there is a corresponding folder data/$DatasetName.
- You can find $FeatureType_$DatasetName.mat in data/$DatasetName, where
$FeatureType = "unigram" or "bigram".
- data/$DatasetName/cv_obj.mat determines the standard evaluation for each dataset (how many
  folds, whats the split, etc.). They are generated by corresponding
  data processing script in src/misc

## Notes
- The datasets are collected by others, please cite the original sources if you work with them
- The data structure used kept the order information of the document, instead of
converting to bag-of-words vector right away. This resulted in some
unnecessary mess for this work, but might make it easier if you want
to try a more complex model.

## Comments
- While many experiments have been ran for this task, performance is
  really all about regularization, and even the simplest model (Naive
  Bayes) would fit the training set perfectly. As far as I know, there is no good
  theory for why things even work in this case of non-sparse weights
  and p>>n.
- Despite a number of highly cited papers that experimented on these same
  datasets, it is unclear if any of the complicated, deep learning models
  today are doing significantly more than bag of words on these datasets:
  - As far as I know, none of these results are impressively better (usually about 1%)
  - Available compute power, engineering competence, and software infrastructure are vastly better for neural models
  - Difference in enthusiasm level: no one seems to try very hard pushing basic models to the available compute power / hardware 
  - These factors makes it difficult to judge whats going on.
- These models run in few seconds or less, and
  behaves predictably for a different test distribution.
- Another [example](http://arxiv.org/abs/1512.02167) of bag of words going strong in 2015.
